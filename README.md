# Implementation-of-K-NN-in-Python-from-scratch


K- Nearest Neighbors is a Supervised learning algorithm that is used for classification as well as regression. The objective of this algorithm is to “assign a class” to newly observed data, i.e., test data in the K distances. K is any positive number. The distances are measured using Euclidean distance. From the K-distances, the classes (of training instances) that occur the most are chosen as the “class” for the given test data. For example, if K is chosen as 3, in the K=3 distance space, if the occurrence of “class 0” is more than any other class, then the test data is assigned as class 0.  

How the algorithm works:
    The Euclidean distance is measured between “a test point” & “every training point”. The distances are stored in a list and sorted later in ascending order. The sorted list is now our K-distances. We can choose any number of K. But choosing K should be balanced. Mind that, the sorted list has nearer distances in them first, if K is chosen as 1, then the probability of the cluster of points in K=1 belonging to the same class is high, therefore even if there were any chances that the “given test data” belongs to any other class will be ignored because only “most occurred/assigned class” is chosen. On the other side, if K takes a higher value, say 20, then there will be many clusters of points belonging to different classes. If the test data falls somewhere around the other class (say class 0) which does not occur frequently as the other classes occur, then the test data will be assigned to the other classes when its actual class is 0. Therefore, choosing K should be balanced well. The accuracy of the model can be calculated from dividing the number of times the model predicted correctly by total number of test data.
